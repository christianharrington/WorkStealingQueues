%!TEX root = ../Work-stealing Queues.tex
\section{Background}
\label{sec:background}

\subsection{Approaches to Concurrency}
The complexity of building concurrent applications arises mostly from the inherent need to manage concurrent access to shared state, in particular shared \emph{mutable} state. With shared mutable state, any thread may write to any memory location at any time (in principle), which makes maintaining the invariants of the system a major concern. Keeping shared state immutable eliminates many such problems, as no threads will ever read different data from the same memory location at different moments in time. While it is arguably easier to reason about concurrent applications with immutable shared state, having mutable state can be desirable for several reasons, the most compelling of which is (possible) increased performance. In the following, we will discuss different approaches to handling concurrent access to shared mutable state.

\todo{Mention a bad approach (single locking queue). Also mention ABA problem.}

% locks
\subsubsection{Lock-based Synchronization}
Locking is a widely used mechanism for handling access to shared mutable state, especially in the realm of imperative programming. Locks can be used to grant exclusive access to one or more shared resources by associating that resource with a lock (in locking schemes with \emph{mutual exclusion locks}, where a given lock can only be held by one thread at a given point in time). The problem of synchronization is solved by a guarantee that no other threads will be accessing the shared resource as long as the lock is being held. Since any access to the resource must be exclusive, a lock must be obtained both when reading from and writing to it. 

While the exclusive locking mechanism preserves the integrity of the shared state, this exclusivity is also one of the greatest disadvantages of locking. Whenever a thread tries to obtain a lock held by another thread, the call blocks and the thread is suspended and must wait (potentially forever) for the current lock owner to finish its work. During this time, the waiting thread cannot do anything else. For locks under high contention, the performance degradation caused by the overhead of suspending and resuming threads might be very high, especially if the lock is only required for reading. For these situations, it can be preferable to use non-blocking synchronization instead.

% blocking vs. non-blocking queues
\subsubsection{Non-blocking Synchronization}
Instead of taking the pessimistic approach of making sure that exclusive access is obtained before performing any operations on a shared resource, non-blocking synchronization is more optimistic in nature\,\citep{Goetz2006}. Being \emph{non-blocking} means that if a thread fails or is suspended, it cannot cause another thread to fail or be suspended, because threads accessing a shared resource never have to wait for another thread (contrary to the lock-based scheme, where a thread may have to wait for the release of a lock)\,\citep{Goetz2006}. Confusingly, a non-blocking algorithm can be said to be \emph{lock-free}, although no locks are involved. In this case, it means that at any point of execution, some thread is making progress.\todo{Rephrase}

In the non-blocking scheme, the integrity of a shared resource is ensured by only allowing updates to happen atomically from a consistent state, where all the invariants of the system hold. If some invariant does not hold, e.g. when a shared variable has another value than expected because it has been updated by another thread, a recovery mechanism is triggered instead. To perform atomic non-blocking updates, specific processor instructions (e.g. \emph{compare-and-swap}) must be used to avoid common concurrency problems such as \emph{check-then-act}. 

Non-blocking synchronization provides a more fine-grained approach to synchronization, where the cost of suspending and resuming threads is replaced by the need for a recovery mechanism when faced with failed updates (retrying is a perfectly valid recovery mechanism). However, a non-blocking algorithm can be more complicated to design than a similar lock-based one\,\citep{Goetz2006}, so using non-blocking synchronization might not always be desirable.

% Here, specific processor instructions (e.g. \emph{compare-and-swap}) are used to perform atomic compound operations on shared resources, but in contrast 

% Non-blocking synchronization utilizes hardware support to ensure the integrity of shared state. Instead of using locks to perform atomic compound operations, specific processor instructions, such as \emph{compare-and-swap}, are used to make sure that updates to shared resources only happen from a consistent state. As opposed to exclusive locking, non-blocking operat

% As opposed to the pessimistic approach of locking, where all locks must be obtained before 
% lock-free vs. wait-free ?


\subsection{Work-stealing Queues} % What is a work-stealing queue?
A work-stealing queue is a queue implementation that facilitates working with work-stealing scheduling algorithms\,\citep{Arora:1998:TSM:277651.277678}. In a work-stealing scheduling algorithm, each thread is assigned a work-stealing queue, the elements of which are suspended computations (denoted ``work''). This queue has the standard \texttt{enqueue} and \texttt{dequeue} operations for adding and removing elements from it, respectively, and only the thread owning the queue has access to these operations. Additionally, the queue exposes a \texttt{steal} operation, through which other threads may ``steal work''.

Work-stealing algorithms are especially well-suited for solving problems where one unit of work may generate more work (examples of which can be found in Section \ref{sec:benchmarks}). The idea is that whenever the work unit processed by a given thread generates more work, this work is stored in the thread's local queue. When the thread finishes its current work, it first examines its own queue, and continues to process any work it might find. If no work is found locally, the thread begins to examine the queues of other threads, in effect ``stealing'' the work generated by these. This design leads to efficient utilization of computation resources, since a thread is never idle: it is either working or looking for more work.
\todo{Add figure showing multiple workers with their own queues}

\subsection{Software Transactional Memory}
Software Transactional Memory (STM) is an attempt to address the difficulty of designing concurrent algorithms and data structures, by enabling the user to specify concurrent operations as transactions. Following the formulation of transactional memory presented by Herlihy and Moss\,\citep{HerlihyMossTM}, Shavit and Touitou proposed the first model for software-only transactional memory\,\citep{ShavitSTM}. Using STM, shared memory can be accessed and updated by means of transactions, where a transaction is understood as an application of a finite sequence of primitive operations to that shared memory. Furthermore, the transactions must have the properties of \emph{atomicity} (transactions appear to happen sequentially) and \emph{serializability} (the sequential order of transactions is consistent with their real-time order)\,\cite{ShavitSTM}.

Ideally, designing concurrent algorithm with STM should shift much of the burden of worrying about concurrency from the user to the runtime system, without incurring any performance trade-offs. A study done by Dice and Shavit\,\citep{Dice06whatreally} shows that algorithms implemented using STM show no performance issues related to scalability when compared to other approaches to concurrency\todo{Rephrase}. However, opponents of STM claim that it causes a higher sequential overhead than more traditional shared-memory approaches\,\citep{Cascaval08}, since transactions expand to many more load and store instructions, leading to performance losses.

In an improvement to their earlier algorithm, Dice, Shalev, and Shavit\,\citep{DiceTLII} present the \emph{Transactional Locking II} algorithm, which supposedly is ``an order of magnitude faster than sequential code made concurrent using a single lock.'' This is achieved using a global version-clock, which allows for fast read-only transactions. Although these findings seem promising, the debate concerning the performance of concurrent algorithms implemented using STM has yet to reach consensus.
